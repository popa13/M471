\chapter{Two Main Laws}

We assume a probability space $(S, \mathcal{A}, P)$ is given.

\section{Law of Averages}

\begin{definition}
If $x_1$, $x_2$, $\ldots$, $x_n$ are real numbers, then the average is defined by
    \[
        \sigma_n := \frac{x_1 + x_2 + \cdots + x_n}{n} .
    \]
\end{definition}

\begin{example}
We throw a fair coin $40$ times and record the result (check \url{www.rolladie.net}). Let $X_j$ be the random variable giving $0$ if it lands on Face and $1$ if it lands on Tale. Compute the average of the $X_j$'s. What do you observe? What number do you think the average should be close to?
\end{example}

In general, in probability, we would like a theorem telling us something like ``if we repeat an experiment many times, then the average of the results approaches the underlying mean value.''

\begin{definition}
We say that the sequence $X_1$, $X_2$, $\ldots$ of random variables \underline{converges in mean} \underline{square to the random variable $X$} if
    \[
        \mathrm{Exp} \big( (X_n - X)^2 \big) \ra 0 \quad \text{ as } n \ra \infty.
    \]
If this holds, then we write ``$X_n \ra X$ in mean-square as $n \ra \infty$''.
\end{definition}

\begin{theorem}[Mean-Square Law of Averages]

Let $X_1$, $X_2$, $\ldots$, $X_n$, $\ldots$ be a list of uncorrelated random variables, each having mean $\mu$ and variance $\sigma^2$. Then,
    \[
        \frac{X_1 + X_2 + \ldots + X_n}{n} \ra \mu \quad \text{(in mean-square.)}
    \]
\end{theorem}

\begin{proof}
Write $S_n = X_1 + X_2 + \ldots + X_n$. Then,
We have
    \[
        \mathrm{Exp} (S_n /n) = \frac{1}{n} \mathrm{Exp} (X_1 + X_2 + \cdots + X_n ) = \frac{1}{n}n \mu = \mu .
    \]
and
    \[
        \mathrm{Exp} \big( (\sigma_n - \mu )^2 \big) = \mathrm{Exp} \Big( \frac{(S_n - n \mu)^2}{n^2} \Big) = \frac{1}{n^2} \mathrm{Var} (S_n ) .
    \]
The random variables are uncorrelated, meaning $\mathrm{Cov} (X_i, X_j ) = 0$, for any $i \neq j$. Therefore,
    \[
        \mathrm{Var} (X_1 + X_2 + \cdots + X_n ) = \mathrm{Var} (X_1) + \mathrm{Var} (X_2) + \cdots + \mathrm{Var} (X_n ) = n \sigma^2 .
    \]
Hence,
    \[
        \mathrm{Exp} \big( (\sigma_n - \mu )^2 \big) = \frac{\sigma^2}{n} \ra 0
    \]
as $n \ra \infty$. Therefore, $\sigma_n \ra \mu$ in mean-square as $n \ra \infty$.
\end{proof}

\subsection*{Convergence in Probability}

\begin{definition}
A sequence of random variable $X_1$, $X_2$, $\ldots$ \underline{converges in probability} to a random variable $X$ if for any $\varepsilon > 0$, 
    \[
        \lim_{n \ra \infty} P (|X_n - X| > \varepsilon ) = 0 .
   \]
\end{definition}

\begin{example}
Let $X_1$, $X_2$, $\ldots$ be a sequence of discrete random variables. Assume that the probability mass function of $X_n$ is given by
    \[
        P (X_n = 0) = 1 - \frac{1}{n} \quad \text{ and } \quad P (X_n = n ) = \frac{1}{n} \quad (n \geq 1) .
    \]
Show that $X_n \ra 0$ in probability as $n \ra \infty$, but $X_n$ does not converge to $0$ in mean square as $n \ra \infty$.
\end{example}

\begin{sol*}
We start with the convergence in probability. For $\varepsilon > 0$ and for any $n > \varepsilon$, we have
    \[
        P (|X_n - 0| > \varepsilon ) = P (X_n = n) = \frac{1}{n} \ra 0 \quad (n \ra \infty ). 
    \]
This implies that $X_n \ra 0$ in probability when $n \ra \infty$. 

As for the mean square convergence, we have
    \[
        \mathrm{Exp} ( (X_n - 0)^2) = E (X_n^2) = (0) \Big( 1 - \frac{1}{n} \Big) + (n^2) \Big( \frac{1}{n} \Big) = n \ra \infty .
    \]
Hence, $X_n$ does not converge in mean square to $0$. 
\end{sol*}

\underline{\textbf{Remark:}} The last example shows that convergence in probability does not imply convergence in mean. 

On the other hand, convergence in mean does imply convergence in probability. This is Theorem \ref{T:MEanQuareImpliesProbability}. To show this result, we need an intermediate result called Chebyshev's inequality.

\begin{theorem}
If $X$ is a random variable and $\mathrm{Exp} (X^2) < \infty$, then for any $t > 0$
    \[
        P (|X| \geq t ) \leq \frac{1}{t^2} \mathrm{Exp} (X^2 ).
    \]
\end{theorem}

Here is a consequence of Chebyshev's inequality.

\begin{theorem}\label{T:MEanQuareImpliesProbability}
If a sequence of random variables $X_1$, $X_2$, $\ldots$ converges in mean-square to a random variable $X$, then it also converges in probability to $X$.
\end{theorem}
\begin{proof}
Assume that $\mathrm{Exp} ( (X_n - X)^2 ) \ra 0$, as $n \ra \infty$. Given $\varepsilon > 0$, from Chebyshev's inequality, we get that
    \[
        P (|X_n - X| > \varepsilon ) \leq \frac{1}{\varepsilon^2} \mathrm{Exp} ( (X_n - X)^2 ) 
    \]
Since $\varepsilon$ is fixed and $\lim_{n \ra \infty} \mathrm{Exp} ( (X_n - X)^2 ) = 0$, we get
    \[
        \lim_{n \ra \infty} P (|X_n - X | > \varepsilon ) = 0 . \qedhere
    \]
\end{proof}


\section{Central Limit Theorem}

Let $X_1$, $X_2$, $\ldots$ be independent random variables with the same means $\mu$ and variance $\sigma^2$. Letting $S_n = X_1 + X_2 + \ldots + X_n$, we have
    \[
        \mathrm{Exp} (S_n) = n \mu \quad \text{and} \quad \mathrm{Var} (S_n) = n \sigma^2 .
    \]
Then, define
    \[
        Z_n = \frac{S_n - \mathrm{Exp} (S_n)}{\sqrt{\mathrm{Var} (S_n)}} = \frac{S_n - n \mu}{\sqrt{n} \sigma} .
    \]
Then, $\mathrm{Exp} (Z_n) = 0$ and $\mathrm{Var} (Z_n) = 1$. These are called the \underline{standardized version} of the sum $S_n$. 

Remarkably, the sequence of random variables $Z_n$ settles to a limit as $n \ra \infty$. Even more impressive is the fact the distribution of the limit can be identified explicitly!

\begin{theorem}
Let $X_1$, $X_2$, $\ldots$ be independent and identically distributed random variables, each with mean $\mu$ and non-zero variable $\sigma^2$. The random variables $Z_n$ satisfies the following:
    \[
        P (Z_n \leq z ) \ra \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{-u^2/2} \, du \quad \forall z \in \mR .
    \]
\end{theorem}

We won't prove this Theorem in this class. It requires a difficult result called the \textit{Continuity Theorem}. We will therefore take for granted the Central Limit Theorem and use it in the next example.

\begin{example}
An unknown fraction $p$ of the population are jedi knights. It is desired to estimate $p$ with error not exceeding $0.005$ by asking a sample of individuals. How large a sample is needed?
\end{example}

\begin{sol*}
We use the Central Limit Theorem. Let $X_i$ be the random variable following a Bernouilli distribution, meaning $X_i = 0$ if the $i$-th person answers no and $X_i = 1$ if that same person answers yes. We have 
    \[
        \mathrm{Exp} (X_i) = p \quad \text{ and } \quad \mathrm{Var} (X_i) = p (1 - p) .
    \]

Set $S_n = X_1 + X_2 + \ldots + X_n$, where $n$ is the size of the sample of individuals. We decide to estimate $p$ with $\hat{p} = S_n /n$. Our goal is to find $n$ such that $|\hat{p} - p| < 0.005$.

The good practice is to reach our goal with a certain degree of certainty. Therefore, we want to find $n$ such 
    \[
        P ( |\hat{p} - p| > 0.005 ) < \alpha 
    \]
where $\alpha$ is called the $p$-value or the tolerance value. In practice, we use $\alpha = 0.05$. Therefore, we want to find $n$ such that 
    \[
        P (| \hat{p} - p | \leq 0.005 ) < 0.95 .
    \]

We can rewrite the previous probability as
    \[
        P (|\hat{p} - p| \leq 0.005 ) = P \left( \frac{|S_n - n p|}{\sqrt{n p (1 - p)}} \leq 0.005\frac{\sqrt{n}}{\sqrt{p (1 - p)}} \right) = P \left( |Z_n| \leq 0.005 \frac{\sqrt{n}}{\sqrt{p (1 - p)}} \right) ,
    \]
where $Z_n = \frac{S_n - \mathrm{Exp} (S_n)}{\sqrt{\mathrm{Var} (S_n)}}$. However, $p (1 - p)$ is unknown and the number $0.005 \frac{\sqrt{n}}{\sqrt{p (1 - p)}}$ is useless. We can see that $p (1 - p) \leq \frac{1}{4}$ and therefore
    \[
        P \left( |Z_n| \leq 0.005 \frac{\sqrt{n}}{\sqrt{p (1 - p)}} \right) \geq P (|Z_n| \leq 0.005 \sqrt{4n} ) .
    \]

From the Central Limit Theorem, if $n$ is large enough, then the distribution of $Z_n$ approaches the distribution of a $Z \sim N (0, 1)$.  Therefore
    \[ 
        P \left( |Z_n| \leq 0.005 \frac{\sqrt{n}}{\sqrt{p (1 - p)}} \right) \gtrapprox P (|Z| \leq 0.005 \sqrt{4n} ) = P (Z \leq 0.005 \sqrt{4n} ) - P (Z \leq -0.005 \sqrt{4n} ) .
    \]
For that probability to be bigger than $0.95$, we have to have $0.005 \sqrt{4n} \geq 1.96$, which means $n \gtrapprox 40,000$. \hfill $\triangle$
\end{sol*}


\begin{comment}

\section{Problems Set}

\subsection*{Mean-Square Law of Large Numbers}

\begin{problem}
Let $X_1, X_2, \ldots$ be a list of random variables which converges to the random variable $X$ in mean square. Show that, for any $a, b \in \mR$, $aX_n + b \ra aX + b$, as $n \ra \infty$.
\end{problem}

\begin{problem}
Let $N_m$ be the number of occurences of $5$ or $6$ in $m$ throws of a fair die. Show that
    \[
        \frac{1}{m} N_m \ra \frac{1}{3} \quad \text{ in mean square}
     \] 
as $m \ra \infty$.
\end{problem}

%\begin{problem}
%Prove the following alternative form of Chebyshev's inequality: If $X$ is a random variable with finite variance and $a > 0$, then
%    \[
%        P \big( |X - \mathrm{Exp} (X)| > a \big) \leq \frac{1}{a^2} \mathrm{Var} (X) .
%    \]
%\end{problem}

%\begin{problem}
%Show that if $X_n \ra X$ in probability, then, as $n \ra \infty$,
%    \[
%        aX_n + b \ra aX + b \quad \text{in probability},
%    \]
%for any $a, b \in \mR$.
%\end{problem}

\subsection*{Central Limit Theorem}

\begin{problem}
The fracture strength of tempered glass averages 14 (measured in thousands of pounds per square inch) and has standard deviation $2$. 
    \begin{enumerate}[label=\alph*)]
    \item What is the probability that the average fracture strength of 100 randomly selected pieces of this glass exceeds 14.5?
    \item Find an interval that includes, with probability 0.95, the average fracture strength of 100 randomly selected pieces of this glass.
    \end{enumerate}
\end{problem}

%\begin{problem}
%A fair die is thrown $12,000$ times. Use the central limit theorem to find values of $a$ and $b$ such that
%    \[
%        P (1900 < S < 2200) \approx \int_a^b \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2} x^2} \, dx ,
%    \]
%where $S$ is the total number of sixes thrown.
%\end{problem}


\section{Solutions to Problems Set}
\setcounter{problem}{0} 

\subsection*{Mean-Square Law of Large Numbers}

\begin{problem}
We have
    \[
        \mathrm{Exp} ((aX_n + b - (aX + b))^2) = \mathrm{Exp} (a^2 (X_n - X)^2) = a^2 \mathrm{Exp} ( (X_n - X)^2 ) .
    \]
By assumption, $\lim_{n \ra \infty} \mathrm{Exp} ( (X_n - X)^2) = 0$, therefore
    \[
        \lim_{n \ra \infty} \mathrm{Exp} ( (aX_n + b - (aX + b))^2) = a^2 \lim_{n \ra \infty} \mathrm{Exp} ( (X_n - X)^2 ) = 0 . 
    \]
Hence, $aX_n + b \ra aX + b$ in mean-square. \hfill $\triangle$
\end{problem}

\begin{problem}
Let $N_m$ be the number of occurences of $5$ or $6$ in $m$ throws of a fair die. Show that
    \[
        \frac{1}{m} N_m \ra \frac{1}{3} \quad \text{ in mean square}
     \] 
as $m \ra \infty$.

Let $X_i$ be the random variable with output $1$ if the die lands on $5$ or $6$ and output $0$ if the die lands on $1, 2, 3$, or $4$. Then, we have
    \[
        \mathrm{Exp} (X_i) = 0 \times \frac{2}{3} + 1 \times \frac{1}{3} = \frac{1}{3}
    \]
and
    \[
        \mathrm{Var} (X_i ) = \mathrm{Exp} (X_i^2) - (\mathrm{Exp} (X_i))^2 = \frac{1}{3} - \frac{1}{9} = \frac{2}{9} .
    \]
Therefore, we get $\mathrm{Exp} (N_m) = \frac{m}{3}$ and $\mathrm{Var} (N_m ) = \frac{2m}{9}$ because in $N_m$ are independent. Hence, we compute
    \[
        \mathrm{Exp} \Big( \Big( \frac{N_m}{m} - \frac{1}{3} \Big)^2\Big) = \mathrm{Exp} \Big( \frac{(N_m - \frac{m}{3})^2}{m^2} \Big) = \frac{1}{m^2} \mathrm{Var} (N_m) = \frac{2}{9m} .
    \]
As $m \ra \infty$, $\frac{2}{9m} \ra 0$ and therefore $N_m/m \ra 1/3$ in mean-square, as $m \ra \infty$. \hfill $\triangle$
\end{problem}

%\begin{problem}
%Prove the following alternative form of Chebyshev's inequality: If $X$ is a random variable with finite variance and $a > 0$, then
%   \[
%       P \big( |X - \mathrm{Exp} (X)| > a \big) \leq \frac{1}{a^2} \mathrm{Var} (X) .
%   \]
%\end{problem}

%\begin{problem}
%Show that if $X_n \ra X$ in probability, then, as $n \ra \infty$,
%   \[
%       aX_n + b \ra aX + b \quad \text{in probability},
%   \]
%for any $a, b \in \mR$.
%\end{problem}

\subsection*{Central Limit Theorem}

\begin{problem}

    \begin{enumerate}[label=\alph*)]
    \item Let $X_i$ be a random variable with output the facture strenght of the $i$-th piece. Then, we have $\mu = \mathrm{Exp} (X_i) = 14$, for any $i$ and $\sigma^2 = \mathrm{Var} (X_i) = 4$ for any $i$. We have $n = 100$, the size of the sample and let $S_n/n = (X_1 + X_2 + \ldots + X_n)/n$ represents the average of the fracture strength in the sample.

    We will use the Central Limit Theorem to estimate the probability
        \[
            P \Big( \frac{S_{100}}{100} > 14.5 \Big) .
        \]
    The standardized version of $S_{100}$ is 
        \[
            Z_{100} = \frac{S_{100} - 100 \mu}{\sqrt{100} \sigma} = \frac{S_{100} - 1400}{20} .
        \]
    We see that
        \[
            \frac{S_{100}}{100} > 14.5 \iff S_{100} > 1450 \iff S_{100} - 1400 > 50 \iff \frac{S_{100} - 1400}{20} > 2.5 .
        \]
    Therefore, 
        \[
            P \Big( \frac{S_{100}}{100} > 14.5 \Big) = P (Z_{100} > 2.5 ) .
        \]
    
    From the Central Limit Theorem,
        \[
            P \Big( \frac{S_{100}}{100} > 14.5 \Big) = P (Z_{100} > 2.5 ) \approx P (Z > 2.5 )
        \]
    where $Z \sim N (0, 1 )$. Using the table of the normal distribution, we find that
        \[
            P (Z > 2.5) = 1 - P (Z \leq 2.5 ) = 1 - 0.99379 = 0.00731 .
        \]
    \item Since the standardized version is centered at the average, we will try to find $a$ such that $S_n/n \in [\mu - a , \mu + a ]$ in $95\%$ of the chances. We want to find $a > 0$ such that
        \[
            P \Big( \left| \frac{S_{100}}{100} - \mu \right| < a \Big) = 0.95 .
        \]
    Rearranging the left-hand side:
        \[
            \frac{S_{100}}{100} - 14 = \frac{S_{100} - 1400}{100} = \frac{2}{10} \Big( \frac{S_{100} - 1400}{20} \Big) = \frac{2}{10} Z_n
        \]
    and therefore
        \[
            P \Big( \left| \frac{S_{100}}{100} - 1400 \right| < a \Big) = P (|Z_n| < 5a ) .
        \]
    Using the Central Limit Theorem, $P (|Z_n| < 5a ) \approx P (|Z| < 5a )$, for $Z \sim N (0, 1)$ and we have to find $a$ such that $P (|Z| < 5a ) = 0.95$. Now, since the normal density of $N (0, 1)$ is symmetric with respect to the $y$-axis, we have $P (Z > 5a ) = 0.025 = P (Z < -5a )$. Therefore, 
        \[
            P (|Z| < 5a ) = 0.95 \iff P (Z < 5a ) = 0.975 .
        \]
    Using the table, we find $z = 1.96$ and therefore $a = 1.96/5 = 0.392$. Hence, the interval containing $S_{100}/100$ in $95\%$ of the chances is $[13.608, 14.392]$. 
    \end{enumerate}
\end{problem}

\end{comment}


